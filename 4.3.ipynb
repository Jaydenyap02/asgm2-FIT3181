{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRNN(nn.Module):\n",
    "    def __init__(self, cell_type='simple_rnn', embed_size=128, state_sizes=[128, 128], output_type=\"mean\", data_manager=None):\n",
    "        super().__init__()\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.output_type = output_type\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size\n",
    "        \n",
    "        # Create an empty ModuleList to store layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Call build to construct the model\n",
    "        self.build()\n",
    "\n",
    "    # Return the corresponding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type='simple_rnn', input_size=128, state_size=128):\n",
    "        if cell_type == 'gru':\n",
    "            return nn.GRU(input_size=input_size, hidden_size=state_size, batch_first=True)\n",
    "        elif cell_type == 'lstm':\n",
    "            return nn.LSTM(input_size=input_size, hidden_size=state_size, batch_first=True)\n",
    "        else:\n",
    "            return nn.RNN(input_size=input_size, hidden_size=state_size, batch_first=True)\n",
    "\n",
    "    def build(self):\n",
    "        # Add embedding layer to the layers list\n",
    "        self.layers.append(nn.Embedding(self.vocab_size, self.embed_size))\n",
    "        \n",
    "        # Create the RNN layers based on state_sizes and append them\n",
    "        input_size = self.embed_size  # Initial input size is the embedding size\n",
    "        for state_size in self.state_sizes:\n",
    "            self.layers.append(self.get_layer(self.cell_type, input_size, state_size))\n",
    "            input_size = state_size  # Update input size for the next layer\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(self.state_sizes[-1], self.data_manager.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the embedding layer\n",
    "        x = self.layers[0](x)  # Embedding\n",
    "        \n",
    "        # Pass through each RNN layer in the model\n",
    "        for layer in self.layers[1:]:\n",
    "            if self.cell_type == 'lstm':\n",
    "                x, (h_n, c_n) = layer(x)\n",
    "            else:\n",
    "                x, h_n = layer(x)\n",
    "        \n",
    "        # Process the output based on the output_type\n",
    "        if self.output_type == \"last_state\":\n",
    "            out = h_n[-1]  # Use the last hidden state of the last RNN layer\n",
    "        elif self.output_type == \"mean\":\n",
    "            out = torch.mean(x, dim=1)  # Take the mean of the hidden states over time\n",
    "        elif self.output_type == \"max\":\n",
    "            out, _ = torch.max(x, dim=1)  # Take the max of the hidden states over time\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown output type: {self.output_type}\")\n",
    "\n",
    "        # Pass through the final fully connected layer for classification\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class RNN(BaseRNN):\n",
    "    def __init__(self, cell_type='gru', embed_size=128, state_sizes=[128, 128], output_type='mean', data_manager=None,\n",
    "                 run_mode='scratch', embed_model='glove-wiki-gigaword-100'):\n",
    "        # Set the run_mode and embed_model attributes first before calling the super constructor\n",
    "        self.run_mode = run_mode\n",
    "        self.embed_model = embed_model\n",
    "\n",
    "        # Create the directory for saving the embeddings if it doesn't exist\n",
    "        if not os.path.exists(\"embeddings\"):\n",
    "            os.makedirs(\"embeddings\")\n",
    "        \n",
    "        # Set the path to save the embedding matrix\n",
    "        self.embed_path = \"embeddings/E.npy\"\n",
    "\n",
    "        # Initialize the embedding size based on the Word2Vec model if not training from scratch\n",
    "        if self.run_mode != 'scratch':\n",
    "            embed_size = int(self.embed_model.split(\"-\")[-1])  # Update embed_size to match pretrained model\n",
    "\n",
    "        # Word to index mapping from the data manager\n",
    "        self.word2idx = data_manager.word2idx\n",
    "        self.vocab_size = data_manager.vocab_size\n",
    "        self.embed_matrix = np.zeros((self.vocab_size, embed_size))\n",
    "\n",
    "        # Proceed with initializing the BaseRNN\n",
    "        super().__init__(cell_type, embed_size, state_sizes, output_type, data_manager)\n",
    "\n",
    "        # If using pretrained embeddings, build the embedding matrix\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.build_embedding_matrix()\n",
    "\n",
    "    def build_embedding_matrix(self):\n",
    "        \"\"\"Build the embedding matrix from pretrained Word2Vec model.\"\"\"\n",
    "        print(f\"Loading {self.embed_model} model...\")\n",
    "        word2vect = api.load(self.embed_model)\n",
    "\n",
    "        # Populate the embedding matrix\n",
    "        for word, idx in self.word2idx.items():\n",
    "            if word in word2vect:\n",
    "                self.embed_matrix[idx] = word2vect[word]\n",
    "            else:\n",
    "                # Randomly initialize words not found in pretrained model\n",
    "                self.embed_matrix[idx] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "        \n",
    "        # Save the embedding matrix to a file\n",
    "        np.save(self.embed_path, self.embed_matrix)\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"Builds the model based on the run mode for embedding initialization.\"\"\"\n",
    "        # If 'scratch', we train embeddings from scratch using nn.Embedding\n",
    "        if self.run_mode == 'scratch':\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        \n",
    "        # If 'init-only', load pretrained embeddings but freeze the embedding layer (no fine-tuning)\n",
    "        elif self.run_mode == 'init-only':\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(self.embed_matrix, dtype=torch.float32), freeze=True)\n",
    "        \n",
    "        # If 'init-fine-tune', load pretrained embeddings and allow fine-tuning\n",
    "        elif self.run_mode == 'init-fine-tune':\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.tensor(self.embed_matrix, dtype=torch.float32), freeze=False)\n",
    "\n",
    "        # Proceed with the rest of the RNN layers as in BaseRNN\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Append embedding layer\n",
    "        self.layers.append(self.embedding)\n",
    "\n",
    "        # Append RNN layers\n",
    "        input_size = self.embed_size\n",
    "        for state_size in self.state_sizes:\n",
    "            self.layers.append(self.get_layer(self.cell_type, input_size, state_size))\n",
    "            input_size = state_size\n",
    "\n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(self.state_sizes[-1], self.data_manager.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "print(\"Running experiment with run_mode = 'init-fine-tune'\")\n",
    "\n",
    "# Define the RNN model with modified dropout and other regularization settings\n",
    "rnn_init_fine_tune = RNN(\n",
    "    cell_type='lstm', \n",
    "    embed_size=128, \n",
    "    state_sizes=[64, 128], \n",
    "    output_type='mean', \n",
    "    data_manager=dm, \n",
    "    run_mode='init-fine-tune'\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer with an updated learning rate\n",
    "optimizer = torch.optim.Adam(rnn_init_fine_tune.parameters(), lr=0.003)\n",
    "\n",
    "# Set up a scheduler for dynamic learning rate adjustment\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Instantiate trainer with early stopping after 3 epochs of no improvement in validation loss\n",
    "trainer_init_fine_tune = BaseTrainer(\n",
    "    model=rnn_init_fine_tune,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    train_loader=dm.train_loader,\n",
    "    val_loader=dm.valid_loader,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "\n",
    "trainer_init_fine_tune.fit(num_epochs=15)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
